# 🏗️Text-to-CAD: Generating CAD Models from Text using Multimodal LLMs

A hybrid neural + procedural approach to generate 2D sketches and 3D CAD models from natural language or image prompts using Multimodal Large Language Models (MLLMs). Developed as part of our CSE499 Capstone Project at North South University.

---

## 👥 Team 

- Md. Rakib Hasan Bhuiyan  
- Ashikur Rahman Ashik  
- Md. Rafid Hasan  
- Masud Shahriar  

---

## 🚀 Project Highlights

- 🔤 **Text/Image → Sketch → 3D CAD Model**
- 🧠 **LLM-powered CAD scripting (OpenSCAD / Fusion360)**
- 🛠️ **Constraint-based refinement & procedural generation**
- 💾 **Export formats:** `.STEP`, `.STL`, `.OBJ`

---

## 🔧 Tools & Models Used

- **LLMs**: GPT-4V, LLaVA, FLAN-T5, LLaMA-3, Vicuna-7B  
- **Vision Models**: BLIP2, Zero123++, ControlNet  
- **Geometry Processing**: PointNet++, MeshCNN  
- **CAD Logic**: SketchGraphs, GeoGPT, Photo2CAD  

---

## 📚 Datasets

- **ShapeNet** – 3D objects + captions  
- **ABC Dataset** – 1M CAD models  
- **Fusion 360 Gallery** – Real-world parametric models  
- **SketchGraphs** – 15M sketches with constraints  
- **Text2Shape** – Text-to-mesh mappings  

---

## 📊 Evaluation Metrics

- Chamfer Distance ↓  
- IoU ↑  
- F-Score ↑  
- Jensen-Shannon Divergence ↓  
- Visual Rating (GPT-4 & Human Review)

---

## 🧪 Project Status

- ✅ Text-to-3D and Image-to-3D models trained  
- 🔄 Pipeline integration in progress  
- 🛠️ CAD scripting via LLMs coming next

---

## 📩 Contact

📧 ashikur.ashik02@northsouth.edu <br>
📧 md.rakib.hasan.bhuiyan@northsouth.edu 


---



